{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJD01Hzu5xGwSothgdpdc2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anna-asmaryan/NSBE-Hacks/blob/email_phishing/Email_phishing_logistic_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies as needed:\n",
        "# pip install kagglehub[pandas-datasets]\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the files you'd like to load\n",
        "file_paths = [\"CEAS_08.csv\", \"Nazario.csv\", \"Nigerian_Fraud.csv\", \"SpamAssasin.csv\"]\n",
        "\n",
        "# Create empty dataframe with sender email, reciever, subject, body, and label\n",
        "df = pd.DataFrame(columns=[\"sender\", \"receiver\", \"date\", \"subject\", \"body\", \"urls\", \"label\"])\n",
        "\n",
        "# Load the latest version\n",
        "for file_path in file_paths:\n",
        "  df_temp = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"naserabdullahalam/phishing-email-dataset\",\n",
        "    file_path,\n",
        "    # Provide any additional arguments like\n",
        "    # sql_query or pandas_kwargs. See the\n",
        "    # documenation for more information:\n",
        "    # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        "  )\n",
        "  df_temp = df_temp.loc[:, [\"sender\", \"receiver\", \"date\", \"subject\", \"body\", \"urls\", \"label\"]]\n",
        "\n",
        "  df = pd.concat([df, df_temp])\n",
        "\n",
        "print(\"First 5 records:\", df.head())\n",
        "print(\"Dimensions: \", df.shape[0])\n",
        "\n",
        "\n",
        "#Check for missing values\n",
        "import numpy as np\n",
        "\n",
        "# Deleting rows with missing body text\n",
        "df = df.dropna(subset=['body'])\n",
        "\n",
        "#Tokenize body text\n",
        "!pip install nltk\n",
        "\n",
        "#import tokenization rsrcs\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from string import punctuation\n",
        "\n",
        "nltk.download('punkt')       # For tokenization\n",
        "nltk.download('stopwords')   # For stopwords\n",
        "nltk.download('wordnet')     # For lemmatization\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize the lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to tokenize, remove stopwords, lemmatize, stem, and join the tokens\n",
        "def tokenize_remove_stopwords_lemmatize_stem_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Get stopwords in English and punctuation\n",
        "    stop_words = set(stopwords.words('english')) | set(punctuation)\n",
        "\n",
        "    # Tokenize, remove stopwords, lemmatize, and then stem\n",
        "    processed_tokens = [\n",
        "        stemmer.stem(lemmatizer.lemmatize(word.lower()))  # Lemmatize and then stem\n",
        "        for word in tokens\n",
        "        if word.lower() not in stop_words  # Remove stopwords and punctuation\n",
        "    ]\n",
        "\n",
        "    # Join the tokens into a single string (with spaces or commas between words)\n",
        "    return ' '.join(processed_tokens)  # You can change the separator if you prefer commas\n",
        "\n",
        "# Apply the processing function to the 'body' column\n",
        "df['body_processed'] = df['body'].apply(tokenize_remove_stopwords_lemmatize_stem_text)\n",
        "\n",
        "# Now, 'body_processed' contains processed text as a string\n",
        "print(df['body_processed'].head())\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Step 1: Split your data into train, validation, and test sets (70% train,\n",
        "# 15% validation, 15% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['body_processed'],\n",
        "                          df['label'], test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5,\n",
        "                                                random_state=42)\n",
        "\n",
        "# Step 2: Vectorize the training, validation, and test sets using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training data\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation and test sets (don't fit again, just transform)\n",
        "X_val_vec = vectorizer.transform(X_val)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV3q1HLCIexh",
        "outputId": "65a14447-2651-499d-ca90-7d3490505213"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "First 5 records:                                               sender  \\\n",
            "0                   Young Esposito <Young@iworld.de>   \n",
            "1                       Mok <ipline's1983@icable.ph>   \n",
            "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
            "3                 Michael Parker <ivqrnai@pobox.com>   \n",
            "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
            "\n",
            "                                         receiver  \\\n",
            "0                     user4@gvc.ceas-challenge.cc   \n",
            "1                   user2.2@gvc.ceas-challenge.cc   \n",
            "2                   user2.9@gvc.ceas-challenge.cc   \n",
            "3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
            "4                   user2.2@gvc.ceas-challenge.cc   \n",
            "\n",
            "                              date  \\\n",
            "0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
            "1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
            "2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
            "3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
            "4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
            "\n",
            "                                             subject  \\\n",
            "0                          Never agree to be a loser   \n",
            "1                             Befriend Jenna Jameson   \n",
            "2                               CNN.com Daily Top 10   \n",
            "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
            "4                         SpecialPricesPharmMoreinfo   \n",
            "\n",
            "                                                body urls label  \n",
            "0  Buck up, your troubles caused by small dimensi...    1     1  \n",
            "1  \\nUpgrade your sex and pleasures with these te...    1     1  \n",
            "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1     1  \n",
            "3  Would anyone object to removing .so from this ...    1     0  \n",
            "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...    1     1  \n",
            "Dimensions:  49860\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    buck troubl caus small dimens soon becom lover...\n",
            "1    upgrad sex pleasur techniqu http //www.brightm...\n",
            "2    +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=...\n",
            "3    would anyon object remov .so list .so tld basi...\n",
            "4    welcomefastshippingcustomersupport http //7iwf...\n",
            "Name: body_processed, dtype: object\n",
            "                                              sender  \\\n",
            "0                   Young Esposito <Young@iworld.de>   \n",
            "1                       Mok <ipline's1983@icable.ph>   \n",
            "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
            "3                 Michael Parker <ivqrnai@pobox.com>   \n",
            "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
            "\n",
            "                                         receiver  \\\n",
            "0                     user4@gvc.ceas-challenge.cc   \n",
            "1                   user2.2@gvc.ceas-challenge.cc   \n",
            "2                   user2.9@gvc.ceas-challenge.cc   \n",
            "3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
            "4                   user2.2@gvc.ceas-challenge.cc   \n",
            "\n",
            "                              date  \\\n",
            "0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
            "1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
            "2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
            "3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
            "4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
            "\n",
            "                                             subject  \\\n",
            "0                          Never agree to be a loser   \n",
            "1                             Befriend Jenna Jameson   \n",
            "2                               CNN.com Daily Top 10   \n",
            "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
            "4                         SpecialPricesPharmMoreinfo   \n",
            "\n",
            "                                                body urls label  \\\n",
            "0  Buck up, your troubles caused by small dimensi...    1     1   \n",
            "1  \\nUpgrade your sex and pleasures with these te...    1     1   \n",
            "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1     1   \n",
            "3  Would anyone object to removing .so from this ...    1     0   \n",
            "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...    1     1   \n",
            "\n",
            "                                      body_processed  \n",
            "0  buck troubl caus small dimens soon becom lover...  \n",
            "1  upgrad sex pleasur techniqu http //www.brightm...  \n",
            "2  +=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=...  \n",
            "3  would anyon object remov .so list .so tld basi...  \n",
            "4  welcomefastshippingcustomersupport http //7iwf...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before fitting the model, convert the target variable to numeric type:\n",
        "y_train = pd.to_numeric(y_train, errors='coerce').astype(int)  # errors='coerce' to handle non-numeric values\n",
        "y_val = pd.to_numeric(y_val, errors='coerce').astype(int)\n",
        "y_test = pd.to_numeric(y_test, errors='coerce').astype(int)\n",
        "\n",
        "# Step 3: Train Logistic Regression model on the training data\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence\n",
        "#is an issue\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the training score\n",
        "training_score = model.score(X_train_vec, y_train)\n",
        "print(f\"Training Accuracy: {training_score}\")\n",
        "\n",
        "# Step 4: Validate the model using the validation set\n",
        "y_val_pred = model.predict(X_val_vec)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiwnZ7EgvpK-",
        "outputId": "e432b25b-bcca-434a-882f-d2c4349ef493"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.9926363141457265\n",
            "Validation Accuracy: 0.9860943976467442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.d(types)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "4NPxaGuV0Plh",
        "outputId": "bb51805a-98f0-4b4a-cdae-2194ae556590"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'd'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-7cd59c95e4fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'd'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Test the model using the test set\n",
        "y_test_pred = model.predict(X_test_vec)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Step 6: Confusion Matrix to evaluate performance\n",
        "print(\"Confusion Matrix for Test Data:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "Vsll4YYVyIjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}